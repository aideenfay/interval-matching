{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications - Fast cohomological cycle matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we reproduce the examples presented in Section 3 of the paper [\"Fast Topological Signal Identification and Persistent Cohomological Cycle Matching\" (García-Redondo, Monod, and Song 2022)](https://arxiv.org/abs/2209.15446) whenever the data used is available for public use. \n",
    "\n",
    "This notebook is intended to be used alongside a High Performance Computer Cluster (HPC) and it follows the next structure:\n",
    "- First we **generate the relevant data**, for which we use the auxiliar script `choose_data.py`. In this step we select the type of application that we intend:\n",
    "    - `matching` for direct application of cycle matching, such as the tracking applications,\n",
    "    - `prevalence` for prevalence application.\n",
    "- After that there is an indication to run the script to **send the jobs to the HPC**. For this step one needs:\n",
    "    1. Scripts to send to the HPC. In the folder of the repository there are examples provided for two workload managers: SLURM and OpenPBS. The files `submit_prevalence.pbs` and `submit_matching.pbs` correspond to the scripts to run for OpenPBS for the prevalence and tracking applications, respectively. The files `exec_prevalence.sh` and `exec_matching.sh` correspond to the scripts to run for SLURM for the prevalence and tracking applications, respectively.\n",
    "    2. The auxiliar python scripts `apply_matching.py` and `apply_prevalent.py`, which are called from the workload manager scripts in order to do matching and prevalence applications, respectively.\n",
    "    3. A compiled version of the modified C++ files in the folders `ripser-tight-representative-cycles` and `simple-ripser-image`. These are due to [1] and [2], respectively, and the version included here is only sligthly altered to retrieve the indices of the simplices associated to the persistence pairs. These indices are obtained after taking a lexicographic refinement of the Vietoris-Rips filtration as explained in [1].\n",
    "- Finally, we **retrieve and process the data** from the previous computations.\n",
    "\n",
    "### References\n",
    "[1] Bauer, Ulrich. 2021. ‘Ripser: Efficient Computation of Vietoris-Rips Persistence Barcodes’. Journal of Applied and Computational Topology 5 (3): 391–423. https://doi.org/10.1007/s41468-021-00071-5.\n",
    "\n",
    "[2] Bauer, Ulrich, and Maximilian Schmahl. 2022. ‘Efficient Computation of Image Persistence’. ArXiv:2201.04170 [Cs, Math], January. http://arxiv.org/abs/2201.04170.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching - tracking applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the type of application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application = 'matching' #'prevalence'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_PH import *\n",
    "from utils_plot import *\n",
    "plt.rcParams['savefig.facecolor']='white' # set background to white in savefig\n",
    "import pickle\n",
    "\n",
    "if application == 'prevalence': \n",
    "    from choose_data import DATASET, generate_data_resamplings, N_ref, N, N_resamp, noise_scale\n",
    "\n",
    "    # temp folder\n",
    "    temp_folder = '{}_temp/'.format(DATASET)\n",
    "    if not os.path.exists(temp_folder) :\n",
    "        os.mkdir(temp_folder)\n",
    "        print('created temp folder for dataset')\n",
    "    else :\n",
    "        raise Exception('Are you sure you want to write on previous temp folder? PLEASE DONT UNACTIVATE THIS MESSAGE')\n",
    "\n",
    "    # generate data + resamplings\n",
    "    full_data, X, list_Y = generate_data_resamplings(dataset = DATASET)\n",
    "    print('dataset :', DATASET)\n",
    "    print('params : ', N_ref, N, N_resamp, noise_scale)\n",
    "\n",
    "    if os.path.exists('{}_temp/X_{}.pkl'.format(DATASET,N_ref)) :\n",
    "        raise Exception('data already generated')\n",
    "\n",
    "    # save data\n",
    "    pickle.dump(full_data, open('{}_temp/full_data.pkl'.format(DATASET), 'wb'))\n",
    "    pickle.dump(X, open('{}_temp/X_{}.pkl'.format(DATASET,N_ref), 'wb'))\n",
    "    pickle.dump(list_Y, open('{}_temp/list_Y_samp{}_{}.pkl'.format(DATASET, N_resamp, N), 'wb'))\n",
    "\n",
    "    print('finished generating data and resamplings')\n",
    "\n",
    "    # compute PH of Xref\n",
    "    out_X = compute_bars_tightreps(X,filename = '{}_temp/ldm_X'.format(DATASET))\n",
    "    bars_X, reps_X, tight_reps_X, indices_X = extract_bars_reps_indices(out_X, only_dim_1 = True)    \n",
    "\n",
    "    res_X = bars_X, reps_X, tight_reps_X, indices_X\n",
    "    # save it\n",
    "    pickle.dump(res_X, open('{}_temp/res_X_{}.pkl'.format(DATASET,N_ref), 'wb'))\n",
    "\n",
    "    # remove unused file\n",
    "    os.remove('{}_temp/ldm_X.lower_distance_matrix'.format(DATASET))\n",
    "\n",
    "    print('finished computing PH of ref data')\n",
    "\n",
    "if application == 'matching':\n",
    "    from choose_data import DATASET, generate_data_matching, N, noise_scale\n",
    "    \n",
    "    # temp folder\n",
    "    temp_folder = '{}_temp/'.format(DATASET)\n",
    "    if not os.path.exists(temp_folder) :\n",
    "        os.mkdir(temp_folder)\n",
    "        print('created temp folder for dataset')\n",
    "    else :\n",
    "        raise Exception('Are you sure you want to write on previous temp folder? PLEASE DONT UNACTIVATE THIS MESSAGE')\n",
    "        \n",
    "    # generate data\n",
    "    full_data, list_X, list_indices = generate_data_matching(dataset = DATASET)\n",
    "    print('dataset :', DATASET)\n",
    "    print('number of points of the samples : ', N)\n",
    "\n",
    "    if os.path.exists('{}_temp/list_X{}.pkl'.format(DATASET,N)) :\n",
    "        raise Exception('data already generated')\n",
    "\n",
    "    # save data\n",
    "    pickle.dump(full_data, open('{}_temp/full_data.pkl'.format(DATASET), 'wb'))\n",
    "    pickle.dump(list_X, open('{}_temp/list_X.pkl'.format(DATASET), 'wb'))\n",
    "\n",
    "    print('finished generating data and resamplings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------ Run the jobs in the HPC now ------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data and visualise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COLLECT RESULTS - prevalence\n",
    "\n",
    "res_X = pickle.load( open( '{}_temp/res_X_{}.pkl'.format(DATASET,N_ref),'rb'))\n",
    "bars_X, reps_X, tight_reps_X, indices_X = res_X\n",
    "bars_reps_X = bars_X, reps_X, tight_reps_X\n",
    "\n",
    "list_matched_X_Y = []\n",
    "list_affinity_X_Y = []\n",
    "list_bars_reps_Y = []\n",
    "\n",
    "for y in range(N_resamp) :\n",
    "\n",
    "    result_i = pickle.load(open('{}_temp/res_match_{}.pkl'.format(DATASET, y), 'rb'))\n",
    "    matched_X_Y, affinity_X_Y, bars_reps_Y = result_i\n",
    "\n",
    "    bars_Y, reps_Y, tight_reps_Y = bars_reps_Y\n",
    "\n",
    "    list_matched_X_Y += [matched_X_Y]\n",
    "    list_affinity_X_Y += [affinity_X_Y]\n",
    "    list_bars_reps_Y += [bars_reps_Y]\n",
    "    \n",
    "# compute prevalence scores for all bars of Xref\n",
    "\n",
    "dim = 1\n",
    "PH1 = bars_X[dim]\n",
    "PH1 = np.array(PH1)\n",
    "ph1 = len(PH1)\n",
    "\n",
    "scores = np.zeros(ph1)\n",
    "\n",
    "for s in range(N_resamp) :\n",
    "    for match, aff in zip(list_matched_X_Y[s],list_affinity_X_Y[s]) :\n",
    "        a,b = match\n",
    "        scores[a] += aff\n",
    "scores /= N_resamp\n",
    "\n",
    "# sort PH1 and scores BUT AFTER COMPUTING scores above\n",
    "arg = argsort(PH1[:,0], option = 'asc')\n",
    "PH1_sorted = PH1[arg]\n",
    "scores_sorted = scores[arg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot AUGMENTED BARCODES of X : with thickness / color = prevalence, length = persistence\n",
    "\n",
    "figpath1 = '{}_temp/{}_barcode_Nref{}_samp{}_N{}.png'.format(DATASET,DATASET, N_ref, N_resamp, N)\n",
    "figpath2 = '{}_temp/{}_augmented_barcode_Nref{}_samp{}_N{}.png'.format(DATASET,DATASET, N_ref, N_resamp, N)\n",
    "\n",
    "plot_bars_PH1(PH1_sorted, scores = None, diagonal = False, delta_y = .2, delta_y_prev = 1, figpath = figpath1)\n",
    "plot_bars_PH1(PH1_sorted, scores = scores_sorted, diagonal = False, delta_y = .2, delta_y_prev = 1, figpath = figpath2)\n",
    "\n",
    "# scatter plot of prevalence scores\n",
    "plt.figure()\n",
    "plt.scatter(np.arange(ph1), scores_sorted)\n",
    "plt.xticks([]) #plt.xticks(np.arange(len(scores)))\n",
    "plt.title('Prevalence scores of cycles in X (sorted by birth time)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('{}_temp/{}_scores_Nref{}_samp{}_N{}.png'.format(DATASET,DATASET, N_ref, N_resamp, N), dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show cycreps on X\n",
    "XT = X[:,[1,0]]\n",
    "XTt = np.zeros(X.shape)\n",
    "XTt[:,0] = X[:,1]\n",
    "XTt[:,1] = -X[:,0]#[::-1]\n",
    "\n",
    "ax = plot_cycreps(XTt, tight_reps_X[dim], ax = None, zoom_factor = 1.5, return_ax = True)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# show stained cycreps on X\n",
    "ax = plot_cycreps_prevalence_2D(XTt, tight_reps_X[dim], scores, plot_points = True,\n",
    "                           zoom_factor = 1.5, maxi_colorbar = None, return_ax = True)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('{}_temp/{}_stained_cyclesX_Nref{}_samp{}_N{}.png'.format(DATASET, DATASET,N_ref, N_resamp, N), dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'actin' in DATASET :\n",
    "    \n",
    "    # plot prevalent cycles + img + pts ?\n",
    "\n",
    "    ax = plot_cycreps_prevalence_2D(XT, tight_reps_X[dim], scores, plot_points = False,\n",
    "                                    maxi_colorbar = None, zoom_factor = 2, return_ax = True)\n",
    "\n",
    "    ## merged resampling point clouds\n",
    "    #xx, yy = whole_Y.T\n",
    "    #ax.scatter(xx, yy, c = '#729dcf', s = 5, alpha = 1)\n",
    "\n",
    "    ##  original img\n",
    "    img, otsu = full_data\n",
    "    ax.imshow(img, cmap = 'gray')\n",
    "    plt.tight_layout()\n",
    "    ## uncomment below to save\n",
    "    plt.savefig('{}_temp/{}_result_Nref{}_samp{}_N{}.png'.format(DATASET,DATASET, N_ref, N_resamp, N), dpi = 300)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'cosmic_web' :\n",
    "    # plot prevalent cycles + galaxies (optional : + filaments (Duque et al., 2022))\n",
    "\n",
    "    ax = plot_cycreps_prevalence_2D(X, tight_reps_X[dim], scores, plot_points = False,\n",
    "                                    maxi_colorbar = None, zoom_factor = 2, return_ax = True)\n",
    "\n",
    "    ## merged resampling point clouds\n",
    "    #xx, yy = whole_Y.T\n",
    "    #ax.scatter(xx, yy, c = '#729dcf', s = 5, alpha = 1)\n",
    "\n",
    "    ##  original galaxy distribution\n",
    "    x,y = full_data.T\n",
    "    ax.scatter(x, y, c = '#729dcf', s = 5, alpha = 1)\n",
    "\n",
    "    ## filaments (load them with previous section)\n",
    "    # ax.scatter(xf,yf,s = 5, c = 'green',alpha = .2)\n",
    "\n",
    "    ## uncomment below to save\n",
    "    #plt.savefig('{}_temp/{}_result_Nref{}_samp{}_N{}.png'.format(DATASET,DATASET,N_ref, N_resamp, N), dpi = 300)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When satisfied with results, **RENAME TEMP FOLDER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching - tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COLLECT RESULTS - matching\n",
    "\n",
    "from utils_PH import *\n",
    "from utils_plot import *\n",
    "plt.rcParams['savefig.facecolor']='white' # set background to white in savefig\n",
    "import pickle\n",
    "\n",
    "\n",
    "from choose_data import DATASET, generate_data_matching, N, noise_scale\n",
    "\n",
    "list_X = pickle.load( open( '{}_temp/list_X.pkl'.format(DATASET),'rb'))\n",
    "full_data = pickle.load(open('{}_temp/full_data.pkl'.format(DATASET), 'rb'))\n",
    "u, thres_mean = full_data\n",
    "\n",
    "list_matched_X_Y = []\n",
    "list_affinity_X_Y = []\n",
    "list_bars_reps_X = []\n",
    "list_bars_reps_Y = []\n",
    "\n",
    "for y in range(len(list_X)-1) :\n",
    "    result_i = pickle.load(open('{}_temp/res_match_{}.pkl'.format(DATASET, y), 'rb'))\n",
    "    \n",
    "    matched_X_Y, affinity_X_Y, bars_reps_X, bars_reps_Y = result_i\n",
    "\n",
    "    list_matched_X_Y += [matched_X_Y]\n",
    "    list_affinity_X_Y += [affinity_X_Y]\n",
    "    list_bars_reps_X += [bars_reps_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track all the cycles in the different slices\n",
    "from utils_PH import*\n",
    "\n",
    "initial_slice = 0 #always for our applications\n",
    "N_slices = 10\n",
    "step = 1\n",
    "\n",
    "# for lateral line: N_slices = , step = 3\n",
    "# for heartbeat and embryogenesis: = N_slices = 10, step = 1\n",
    "\n",
    "cycles_tracked = {}\n",
    "affinities_tracked = {}\n",
    "list_indices =[i for i in range(step, N_slices + initial_slice -  step, step)]\n",
    "\n",
    "from copy import deepcopy\n",
    "list_matched_copy = deepcopy(list_matched_X_Y)\n",
    "\n",
    "for i in range(initial_slice, N_slices + initial_slice - step, step):\n",
    "    cycles_tracked[i] = []\n",
    "    affinities_tracked[i] = []\n",
    "    if i > initial_slice:\n",
    "        list_indices.remove(i)\n",
    "    for j, match in enumerate(list_matched_copy[i]):\n",
    "        cycle = match[0]\n",
    "        tracked_cycle, tracked_affinity = \\\n",
    "            track_cycles_from_slice(list_matched_X_Y, list_affinity_X_Y, cycle, list_indices, initial_slice = i)\n",
    "        length_track = len(tracked_cycle)\n",
    "        if length_track > 1 :\n",
    "            cycles_tracked[i] += [tracked_cycle]\n",
    "            affinities_tracked[i] += [tracked_affinity]\n",
    "        if length_track > 1:\n",
    "            for k, j in zip(range(i, N_slices + initial_slice -  step, step),  range(length_track)):\n",
    "                list_matched_copy[k].remove(tracked_cycle[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a single list with all the information for the plots\n",
    "list_bars = {}\n",
    "list_reps = {}\n",
    "list_tight_reps = {}\n",
    "\n",
    "for i in range(initial_slice, initial_slice + N_slices  -step, step):\n",
    "    list_bars[i], list_reps[i], list_tight_reps[i] = list_bars_reps_X[i]\n",
    "\n",
    "out_last_slice = compute_bars_tightreps(list_X[N_slices - 1])\n",
    "list_bars_reps_last_slice = extract_bars_reps(out_last_slice)\n",
    "list_bars[N_slices - 1], list_reps[N_slices - 1], list_tight_reps[N_slices - 1] = list_bars_reps_last_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of tunnels and generate colors accordingly\n",
    "n = 0\n",
    "for i in range(initial_slice, initial_slice + N_slices-step,step):\n",
    "    n += len(cycles_tracked[i])\n",
    "\n",
    "import random\n",
    "no_of_colors= n\n",
    "colors =[\"#\"+''.join([random.choice('0123456789ABCDEF') for i in range(6)]) \n",
    "       for j in range(no_of_colors)]\n",
    "for j in range(no_of_colors):\n",
    "    plt.scatter(random.randint(0,10),random.randint(0,10),c=colors[j],s=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PLOT FOR THE LATERAL LINE (FOR INSTANCE)\n",
    "\n",
    "tracked = cycles_tracked[0][0]\n",
    "length_track = len(tracked)\n",
    "u, thres_otsu = full_data\n",
    "images = u\n",
    "list_cycreps = list_tight_reps\n",
    "dim = 1\n",
    "\n",
    "\n",
    "# some pre-selected colors\n",
    "#colors = ['indianred', 'maroon', 'salmon', 'orangered', 'sandybrown', 'bisque', 'goldenrod', 'gold', 'yellow', 'olivedrab',\n",
    "#        'greenyellow', 'darkseagreen', 'palegreen', 'forestgreen', 'turquoise', 'teal', 'slategray', 'lightblue', 'steelblue', \n",
    "#        'blue', 'mediumslateblue', 'blueviolet', 'violet', 'purple', 'magenta', 'hotpink', 'crimson', 'palevioletred','skyblue', \n",
    "#        'blue', 'pink', 'red', 'green', 'yellow', 'cyan', 'olive' ]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(3, 5, figsize = (20,12))\n",
    "\n",
    "row = -1\n",
    "c = 0\n",
    "for j in range(len(list_X)-1):\n",
    "    #print(j)\n",
    "    #print(j, j%5)\n",
    "    if j % 5 == 0:\n",
    "        row += 1\n",
    "    ax[row,j%5].imshow(images[j*3, :, :], cmap = 'gray', origin = 'lower')\n",
    "    ax[row,j%5].set_aspect('equal')\n",
    "    num_cycles = len(cycles_tracked[j])\n",
    "    for k in range(num_cycles):\n",
    "        tracked = cycles_tracked[j][k]\n",
    "        for i, match in enumerate(tracked):\n",
    "            index_cycle = match[0]\n",
    "            cycrep = list_cycreps[(j+i)][dim][index_cycle]\n",
    "            xx = list_X[3*(j+i)][:,0]\n",
    "            yy = list_X[3*(j+i)][:,1]\n",
    "            if len(cycrep) > 0 :\n",
    "                cycrep = np.array(cycrep)\n",
    "                ax[((j+i)-(j+i)%5)//5,(j+i)%5].plot(yy[cycrep.T], xx[cycrep.T], c = colors[c], alpha = 0.7, linewidth = 2)\n",
    "            if j + i == 13:\n",
    "                #print(match)\n",
    "                index_cycle = match[1]\n",
    "                cycrep = list_cycreps[(j+i+1)][dim][index_cycle]\n",
    "                #print(cycrep)\n",
    "                xx = list_X[3*(j+i+1)][:,0]\n",
    "                yy = list_X[3*(j+i+1)][:,1]\n",
    "                cycrep = np.array(cycrep)\n",
    "                ax[((j+i)-(j+i)%5)//5,(j+i)%5+1].plot(yy[cycrep.T], xx[cycrep.T], c = colors[c], alpha = 0.7, linewidth = 2)\n",
    "                    \n",
    "        c += 1\n",
    "ax[2,4].imshow(images[14*3, :, :], cmap = 'gray', origin = 'lower')\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        ax[i,j].tick_params(bottom = False, left = False, labelbottom = False, labelleft = False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b377582f1cd1ad79b8391855433a7f9f818b52bce68a2ff1d3c94dbef4f604d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
